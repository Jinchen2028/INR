train_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./data/DIV2K/train/HR
      repeat: 20
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled
    args:
      inp_size: 48
      scale_max: 4
      augment: true
      sample_q: 2304 #48
  batch_size: 16

val_dataset:
  dataset:
    name: image-folder
    args:
      root_path: ./data/DIV2K/val/HR
      first_k: 10  #表示只用数据集前10个
      repeat: 160
      cache: in_memory
  wrapper:
    name: sr-implicit-downsampled
    args:
      inp_size: 48
      scale_max: 4
      sample_q: 2304
  batch_size: 16

data_norm:
  inp: {sub: [0.5], div: [0.5]}
  gt: {sub: [0.5], div: [0.5]}

model:
  name: liif
  args:
    encoder_spec:
      name: edsr-baseline
      args:
        no_upsampling: true
    imnet_spec:
      name: mlp
      args:
        out_dim: 3
        hidden_list: [256, 256, 256, 256]

#resume: /home/jinchen/TEST/liif/save/Train-edsr-liif/epoch-best.pth

optimizer:
  name: adam
  args:
    lr: 5.e-5

#scheduler:
#    type: CosineAnnealingRestartLR
#    periods: [ 1000 ]
#    restart_weights: [ 1 ]
#    eta_min: !!float 1e-6

warmup_step_lr:
  multiplier: 10 #结束后把lr * 这个参数，作为之后优化器初始学习率
  total_epoch: 2
  use_warmup: true

epoch_max: 7 #sched + warmup

scheduler:
    type: MultiStepLR
    milestones: [2, 3, 4] #这个与warmup的epoch无关
    gamma: 0.5
    periods: [ 5 ]

#  scheduler:
#    type: MultiStepLR
#    milestones: [ 200000, 400000, 600000, 800000 ]
#    gamma: 0.5

epoch_val: 1
epoch_save: 100
